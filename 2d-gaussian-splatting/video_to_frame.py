# video_to_2dgs_improved.py
import cv2
import os
import json
import numpy as np
from tqdm import tqdm
from pathlib import Path
from PIL import Image
import shutil
import sys
import math

def extract_frames_uniform(video_path, output_dir, target_frames=300, quality=95):
    """
    ä»è§†é¢‘å‡åŒ€æå–å¸§ï¼Œç¡®ä¿æ—¶é—´å‡åŒ€åˆ†å¸ƒ
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶å®Œæ•´è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        target_frames: ç›®æ ‡å¸§æ•°ï¼ˆæœ€å¤š300ï¼‰
        quality: JPGè´¨é‡ (1-100)
    """
    print("="*60)
    print("2D-GS è§†é¢‘å¤„ç†è„šæœ¬ (æ”¹è¿›ç‰ˆ - å‡åŒ€æŠ½å¸§)")
    print("="*60)
    
    # æ£€æŸ¥è§†é¢‘æ–‡ä»¶
    video_path = Path(video_path)
    if not video_path.exists():
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        
        # å°è¯•æŸ¥æ‰¾è§†é¢‘
        print("\nğŸ” å°è¯•æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶...")
        possible_locations = [
            "/datadisk/home/cv25_010/code/cv_project/item_1.mp4",
            "/home/cv25_010/code/cv_project/item_1.mp4",
            "item_1.mp4",
            "/home/cv25_010/cv_project/item_1.mp4",
        ]
        
        for loc in possible_locations:
            if os.path.exists(loc):
                video_path = Path(loc)
                print(f"âœ… æ‰¾åˆ°è§†é¢‘: {video_path}")
                break
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
            print("è¯·å°†è§†é¢‘æ–‡ä»¶æ”¾åœ¨ä»¥ä¸‹ä½ç½®ä¹‹ä¸€:")
            for loc in possible_locations:
                print(f"  - {loc}")
            return None, None
    
    print(f"ğŸ“¹ è§†é¢‘æ–‡ä»¶: {video_path}")
    
    # åˆ›å»ºBlenderæ ¼å¼ç›®å½•ç»“æ„
    base_dir = Path(output_dir)
    train_dir = base_dir / "train"  # å¿…é¡»å«trainï¼Œä¸æ˜¯images
    test_dir = base_dir / "test"    # æµ‹è¯•å›¾ç‰‡ç›®å½•
    
    for dir_path in [train_dir, test_dir]:
        dir_path.mkdir(parents=True, exist_ok=True)
    
    # æ‰“å¼€è§†é¢‘
    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened():
        print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
        return None, None
    
    # è·å–è§†é¢‘ä¿¡æ¯
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps if fps > 0 else 0
    
    print(f"\nğŸ“Š è§†é¢‘ä¿¡æ¯:")
    print(f"  FPS: {fps:.2f}")
    print(f"  æ€»å¸§æ•°: {total_frames}")
    print(f"  æ—¶é•¿: {duration:.2f}ç§’")
    print(f"  ç›®æ ‡å¸§æ•°: {target_frames}")
    
    # è®¡ç®—å‡åŒ€é‡‡æ ·é—´éš”
    frame_interval = max(1, total_frames // target_frames)
    actual_target = min(target_frames, total_frames // frame_interval)
    
    print(f"  è®¡ç®—å¸§é—´éš”: {frame_interval}")
    print(f"  é¢„è®¡æå–: {actual_target} å¼ ")
    
    # è®¡ç®—é‡‡æ ·å¸§ç´¢å¼•
    frame_indices = np.linspace(0, total_frames-1, actual_target, dtype=int)
    
    saved_count = 0
    saved_files = []
    
    # è¿›åº¦æ¡
    pbar = tqdm(total=actual_target, desc="å‡åŒ€æå–å¸§")
    
    for idx, frame_idx in enumerate(frame_indices):
        # è·³è½¬åˆ°æŒ‡å®šå¸§
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        
        if not ret:
            continue
        
        # ç”Ÿæˆæ–‡ä»¶å - æŒ‰ç…§Blenderæ ¼å¼: r_{æ•°å­—}.png
        frame_filename = f"r_{saved_count:04d}.png"
        output_path = train_dir / frame_filename
        
        # ä¿å­˜ä¸ºPNGï¼ˆBlenderæ ¼å¼ç”¨PNGï¼‰
        # è½¬æ¢BGRåˆ°RGB
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # ä¿å­˜
        img = Image.fromarray(frame_rgb)
        img.save(output_path, "PNG", optimize=True)
        
        saved_files.append({
            "index": saved_count,
            "filename": frame_filename,
            "path": str(output_path),
            "original_frame": frame_idx,
            "time_sec": frame_idx / fps
        })
        saved_count += 1
        pbar.update(1)
    
    cap.release()
    pbar.close()
    
    if saved_count == 0:
        print("âŒ æ²¡æœ‰æå–åˆ°ä»»ä½•å¸§")
        return None, None
    
    print(f"\nâœ… æå–å®Œæˆ:")
    print(f"  å‡åŒ€æå–äº† {saved_count} å¼ PNGå›¾ç‰‡")
    print(f"  ä¿å­˜åœ¨: {train_dir}")
    
    # æ˜¾ç¤ºæ—¶é—´åˆ†å¸ƒ
    if saved_files:
        times = [f["time_sec"] for f in saved_files]
        print(f"  æ—¶é—´èŒƒå›´: {times[0]:.1f}ç§’ åˆ° {times[-1]:.1f}ç§’")
        print(f"  å¹³å‡é—´éš”: {(times[-1]-times[0])/(len(times)-1):.2f}ç§’")
        
        # æ˜¾ç¤ºç¬¬ä¸€å¼ å›¾ç‰‡ä¿¡æ¯
        first_file = train_dir / saved_files[0]["filename"]
        img = Image.open(first_file)
        print(f"  å›¾ç‰‡å°ºå¯¸: {img.size[0]}x{img.size[1]}")
        print(f"  æ ¼å¼: {img.format}")
    
    return base_dir, saved_files

def create_precise_camera_poses(base_dir, image_files, camera_angle_x=0.6911112070083618, 
                               radius=3.0, height=1.5, look_at=(0, 0, 0)):
    """
    åˆ›å»ºç²¾ç¡®çš„åœ†å½¢ç›¸æœºè½¨è¿¹
    
    Args:
        base_dir: åŸºç¡€ç›®å½•
        image_files: å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨
        camera_angle_x: ç›¸æœºæ°´å¹³è§†è§’ï¼ˆå¼§åº¦ï¼‰
        radius: ç›¸æœºè½¨é“åŠå¾„
        height: ç›¸æœºé«˜åº¦
        look_at: çœ‹å‘çš„ç‚¹åæ ‡
    """
    print("\nğŸ“„ åˆ›å»ºç²¾ç¡®ç›¸æœºä½å§¿...")
    
    base_dir = Path(base_dir)
    train_dir = base_dir / "train"
    
    if not image_files:
        print("âŒ æ²¡æœ‰å›¾ç‰‡æ–‡ä»¶")
        return False
    
    # è·å–å›¾ç‰‡å°ºå¯¸
    first_file = train_dir / image_files[0]["filename"]
    img = Image.open(first_file)
    width, height_px = img.size
    
    print(f"  å›¾ç‰‡å°ºå¯¸: {width}x{height_px}")
    print(f"  ç›¸æœºè§’åº¦: {camera_angle_x} å¼§åº¦ ({np.degrees(camera_angle_x):.1f}Â°)")
    print(f"  è½¨é“åŠå¾„: {radius}")
    print(f"  ç›¸æœºé«˜åº¦: {height}")
    print(f"  çœ‹å‘ç‚¹: {look_at}")
    print(f"  æ€»å¸§æ•°: {len(image_files)}")
    
    # åˆ›å»ºtransforms_train.json
    transforms_train = {
        "camera_angle_x": camera_angle_x,
        "frames": []
    }
    
    look_at = np.array(look_at)
    
    # ä¸ºæ¯å¼ è®­ç»ƒå›¾ç‰‡åˆ›å»ºç›¸æœºä½å§¿
    for i, img_info in enumerate(image_files):
        # è®¡ç®—è§’åº¦ (å‡åŒ€åˆ†å¸ƒ0-360åº¦)
        angle = 2 * np.pi * i / len(image_files)
        
        # ç›¸æœºä½ç½® (åœ†å½¢è½¨è¿¹)
        x = radius * np.cos(angle)
        y = height
        z = radius * np.sin(angle)
        
        # ç›¸æœºä½ç½®å‘é‡
        eye = np.array([x, y, z])
        
        # è®¡ç®—çœ‹å‘ç›®æ ‡çš„å˜æ¢çŸ©é˜µ
        # 1. è®¡ç®—å‰å‘å‘é‡ (ä»ç›¸æœºæŒ‡å‘ç›®æ ‡)
        forward = look_at - eye
        forward = forward / np.linalg.norm(forward)
        
        # 2. åˆå§‹ä¸Šå‘é‡
        world_up = np.array([0, 1, 0])
        
        # 3. è®¡ç®—å³å‘é‡
        right = np.cross(forward, world_up)
        # å¦‚æœå³å‘é‡é•¿åº¦ä¸º0ï¼Œè¯´æ˜forwardå’Œworld_upå¹³è¡Œ
        if np.linalg.norm(right) < 1e-6:
            right = np.array([1, 0, 0])  # ä½¿ç”¨é»˜è®¤å³å‘é‡
        right = right / np.linalg.norm(right)
        
        # 4. é‡æ–°è®¡ç®—ä¸Šå‘é‡ä»¥ç¡®ä¿æ­£äº¤
        up = np.cross(right, forward)
        up = up / np.linalg.norm(up)
        
        # åˆ›å»º4x4ç›¸æœºåˆ°ä¸–ç•Œå˜æ¢çŸ©é˜µ
        # æ³¨æ„ï¼šåœ¨NeRF/Blenderæ ¼å¼ä¸­ï¼Œè¿™æ˜¯ç›¸æœºåˆ°ä¸–ç•Œçš„å˜æ¢
        transform = np.eye(4)
        transform[:3, 0] = right      # å³å‘é‡
        transform[:3, 1] = up         # ä¸Šå‘é‡  
        transform[:3, 2] = -forward   # å‰å‘é‡ï¼ˆå–åï¼Œå› ä¸ºç›¸æœºåæ ‡ç³»zå‘å‰ï¼‰
        transform[:3, 3] = eye        # ä½ç½®
        
        frame = {
            "file_path": f"./train/{img_info['filename'].replace('.png', '')}",
            "rotation": 0.012566370614359171,  # æ ‡å‡†å€¼
            "transform_matrix": transform.tolist()
        }
        transforms_train["frames"].append(frame)
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºå‰å‡ ä¸ªç›¸æœºçš„ä½ç½®
        if i < 3:
            print(f"  ç›¸æœº {i}: ä½ç½®({x:.2f}, {y:.2f}, {z:.2f}), è§’åº¦{np.degrees(angle):.1f}Â°")
    
    # ä¿å­˜transforms_train.json
    transforms_train_file = base_dir / "transforms_train.json"
    with open(transforms_train_file, 'w') as f:
        json.dump(transforms_train, f, indent=2)
    
    print(f"âœ… åˆ›å»º: {transforms_train_file}")
    print(f"  è®­ç»ƒå¸§æ•°: {len(transforms_train['frames'])}")
    
    # åˆ›å»ºtransforms_test.jsonï¼ˆå‡åŒ€é€‰æ‹©æµ‹è¯•å¸§ï¼‰
    transforms_test = {
        "camera_angle_x": camera_angle_x,
        "frames": []
    }
    
    test_dir = base_dir / "test"
    test_dir.mkdir(exist_ok=True)
    
    # å‡åŒ€é€‰æ‹©æµ‹è¯•å¸§ (å¤§çº¦10%çš„è®­ç»ƒå¸§)
    num_test = max(5, len(image_files) // 10)
    test_indices = np.linspace(0, len(image_files)-1, num_test, dtype=int)
    
    print(f"  é€‰æ‹© {num_test} å¼ æµ‹è¯•å¸§: {list(test_indices)}")
    
    for idx in test_indices:
        if idx < len(image_files):
            img_info = image_files[idx]
            
            # ä½¿ç”¨ç¨å¾®ä¸åŒçš„è§’åº¦ï¼ˆåç§»10åº¦ï¼‰
            angle_offset = np.radians(10)
            angle = 2 * np.pi * idx / len(image_files) + angle_offset
            
            x = radius * np.cos(angle)
            y = height
            z = radius * np.sin(angle)
            
            eye = np.array([x, y, z])
            forward = look_at - eye
            forward = forward / np.linalg.norm(forward)
            world_up = np.array([0, 1, 0])
            right = np.cross(forward, world_up)
            if np.linalg.norm(right) < 1e-6:
                right = np.array([1, 0, 0])
            right = right / np.linalg.norm(right)
            up = np.cross(right, forward)
            up = up / np.linalg.norm(up)
            
            transform = np.eye(4)
            transform[:3, 0] = right
            transform[:3, 1] = up
            transform[:3, 2] = -forward
            transform[:3, 3] = eye
            
            frame = {
                "file_path": f"./test/{img_info['filename'].replace('.png', '')}",
                "rotation": 0.012566370614359171,
                "transform_matrix": transform.tolist()
            }
            transforms_test["frames"].append(frame)
            
            # å¤åˆ¶å›¾ç‰‡åˆ°testç›®å½•
            src = train_dir / img_info["filename"]
            dst = test_dir / img_info["filename"]
            shutil.copy2(src, dst)
    
    transforms_test_file = base_dir / "transforms_test.json"
    with open(transforms_test_file, 'w') as f:
        json.dump(transforms_test, f, indent=2)
    
    print(f"âœ… åˆ›å»º: {transforms_test_file}")
    print(f"  æµ‹è¯•å¸§æ•°: {len(transforms_test['frames'])}")
    
    # åˆ›å»ºç›¸æœºè½¨è¿¹å¯è§†åŒ–
    create_camera_trajectory_visualization(transforms_train, base_dir)
    
    return True

def create_camera_trajectory_visualization(transforms_data, output_dir):
    """åˆ›å»ºç›¸æœºè½¨è¿¹å¯è§†åŒ–"""
    print("\nğŸ“ˆ åˆ›å»ºç›¸æœºè½¨è¿¹å¯è§†åŒ–...")
    
    # æå–æ‰€æœ‰ç›¸æœºä½ç½®
    positions = []
    for frame in transforms_data["frames"]:
        transform = np.array(frame["transform_matrix"])
        position = transform[:3, 3]
        positions.append(position)
    
    positions = np.array(positions)
    
    # åˆ›å»º3Dè½¨è¿¹å›¾
    from mpl_toolkits.mplot3d import Axes3D
    
    fig = plt.figure(figsize=(12, 10))
    
    # 3Dè½¨è¿¹
    ax1 = fig.add_subplot(221, projection='3d')
    ax1.plot(positions[:, 0], positions[:, 1], positions[:, 2], 'b-', alpha=0.6, linewidth=1)
    ax1.scatter(positions[:, 0], positions[:, 1], positions[:, 2], c=range(len(positions)), 
               cmap='viridis', s=20, alpha=0.8)
    ax1.set_xlabel('X')
    ax1.set_ylabel('Y')
    ax1.set_zlabel('Z')
    ax1.set_title('ç›¸æœº3Dè½¨è¿¹')
    ax1.grid(True, alpha=0.3)
    
    # XYæŠ•å½±
    ax2 = fig.add_subplot(222)
    ax2.plot(positions[:, 0], positions[:, 1], 'b-', alpha=0.6, linewidth=1)
    ax2.scatter(positions[:, 0], positions[:, 1], c=range(len(positions)), 
               cmap='viridis', s=20, alpha=0.8)
    ax2.set_xlabel('X')
    ax2.set_ylabel('Y')
    ax2.set_title('XYå¹³é¢æŠ•å½±')
    ax2.axis('equal')
    ax2.grid(True, alpha=0.3)
    
    # XZæŠ•å½±
    ax3 = fig.add_subplot(223)
    ax3.plot(positions[:, 0], positions[:, 2], 'b-', alpha=0.6, linewidth=1)
    ax3.scatter(positions[:, 0], positions[:, 2], c=range(len(positions)), 
               cmap='viridis', s=20, alpha=0.8)
    ax3.set_xlabel('X')
    ax3.set_ylabel('Z')
    ax3.set_title('XZå¹³é¢æŠ•å½±')
    ax3.axis('equal')
    ax3.grid(True, alpha=0.3)
    
    # é«˜åº¦å˜åŒ–
    ax4 = fig.add_subplot(224)
    frames = range(len(positions))
    ax4.plot(frames, positions[:, 1], 'g-', linewidth=2)
    ax4.set_xlabel('å¸§åºå·')
    ax4.set_ylabel('Y (é«˜åº¦)')
    ax4.set_title('ç›¸æœºé«˜åº¦å˜åŒ–')
    ax4.grid(True, alpha=0.3)
    
    plt.suptitle(f'ç›¸æœºè½¨è¿¹åˆ†æ ({len(positions)}ä¸ªç›¸æœºä½ç½®)', fontsize=14)
    plt.tight_layout()
    
    # ä¿å­˜
    output_path = output_dir / "camera_trajectory.png"
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… ç›¸æœºè½¨è¿¹å›¾: {output_path}")
    
    # ä¿å­˜è½¨è¿¹æ•°æ®
    traj_data = {
        "num_cameras": len(positions),
        "positions": positions.tolist(),
        "radius_avg": np.mean(np.sqrt(positions[:, 0]**2 + positions[:, 2]**2)),
        "height_avg": np.mean(positions[:, 1]),
        "height_std": np.std(positions[:, 1])
    }
    
    with open(output_dir / "camera_trajectory.json", 'w') as f:
        json.dump(traj_data, f, indent=2)
    
    print(f"  å¹³å‡è½¨é“åŠå¾„: {traj_data['radius_avg']:.2f}")
    print(f"  å¹³å‡é«˜åº¦: {traj_data['height_avg']:.2f} Â± {traj_data['height_std']:.2f}")

def analyze_video_for_best_params(video_path):
    """åˆ†æè§†é¢‘ä»¥ç¡®å®šæœ€ä½³å‚æ•°"""
    print("\nğŸ” åˆ†æè§†é¢‘ç¡®å®šæœ€ä½³å‚æ•°...")
    
    cap = cv2.VideoCapture(str(video_path))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps
    
    print(f"  è§†é¢‘æ—¶é•¿: {duration:.1f}ç§’")
    print(f"  æ€»å¸§æ•°: {total_frames}")
    
    # è¯»å–å‡ å¸§åˆ†æè¿åŠ¨
    sample_frames = min(10, total_frames)
    frame_indices = np.linspace(0, total_frames-1, sample_frames, dtype=int)
    
    prev_frame = None
    motion_scores = []
    
    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        
        if not ret:
            continue
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        if prev_frame is not None:
            # è®¡ç®—å¸§é—´å·®å¼‚
            diff = cv2.absdiff(gray, prev_frame)
            motion_score = np.mean(diff)
            motion_scores.append(motion_score)
        
        prev_frame = gray
    
    cap.release()
    
    if motion_scores:
        avg_motion = np.mean(motion_scores)
        print(f"  å¹³å‡è¿åŠ¨åˆ†æ•°: {avg_motion:.1f}")
        
        # æ ¹æ®è¿åŠ¨ç¡®å®šç›®æ ‡å¸§æ•°
        if avg_motion > 50:  # å¿«é€Ÿè¿åŠ¨
            target_frames = 300
            print(f"  æ£€æµ‹åˆ°å¿«é€Ÿè¿åŠ¨ï¼Œæ¨èå¸§æ•°: {target_frames}")
        elif avg_motion > 20:  # ä¸­ç­‰è¿åŠ¨
            target_frames = 200
            print(f"  æ£€æµ‹åˆ°ä¸­ç­‰è¿åŠ¨ï¼Œæ¨èå¸§æ•°: {target_frames}")
        else:  # æ…¢é€Ÿè¿åŠ¨
            target_frames = 150
            print(f"  æ£€æµ‹åˆ°æ…¢é€Ÿè¿åŠ¨ï¼Œæ¨èå¸§æ•°: {target_frames}")
    else:
        target_frames = 200
        print(f"  ä½¿ç”¨é»˜è®¤å¸§æ•°: {target_frames}")
    
    return min(target_frames, 300)  # æœ€å¤š300å¸§

def create_quality_report(base_dir, image_files, video_info):
    """åˆ›å»ºè´¨é‡æŠ¥å‘Š"""
    print("\nğŸ“Š åˆ›å»ºæ•°æ®å¤„ç†æŠ¥å‘Š...")
    
    report = {
        "video_info": video_info,
        "extraction_info": {
            "total_frames_extracted": len(image_files),
            "frame_indices": [f["original_frame"] for f in image_files],
            "time_points": [f["time_sec"] for f in image_files],
            "time_span": f"{image_files[0]['time_sec']:.1f}s - {image_files[-1]['time_sec']:.1f}s"
        },
        "camera_config": {
            "num_train_frames": len(image_files),
            "camera_angle_x": 0.6911112070083618,
            "trajectory_radius": 3.0,
            "camera_height": 1.5
        },
        "quality_metrics": {
            "time_uniformity": "ä¼˜" if len(image_files) > 100 else "è‰¯",
            "frame_coverage": f"{(len(image_files) / video_info['total_frames'] * 100):.1f}%",
            "recommended_iterations": 50000 if len(image_files) > 200 else 30000
        }
    }
    
    report_path = base_dir / "processing_report.json"
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… å¤„ç†æŠ¥å‘Š: {report_path}")
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    md_report = f"""# 2D-GS è§†é¢‘å¤„ç†æŠ¥å‘Š

## è§†é¢‘ä¿¡æ¯
- æ–‡ä»¶: {video_info['path']}
- æ—¶é•¿: {video_info['duration']:.1f}ç§’
- æ€»å¸§æ•°: {video_info['total_frames']}
- FPS: {video_info['fps']:.2f}

## æŠ½å¸§ç»“æœ
- æå–å¸§æ•°: {len(image_files)}
- æ—¶é—´èŒƒå›´: {report['extraction_info']['time_span']}
- å¸§è¦†ç›–: {report['quality_metrics']['frame_coverage']}
- æ—¶é—´å‡åŒ€æ€§: {report['quality_metrics']['time_uniformity']}

## ç›¸æœºé…ç½®
- è®­ç»ƒå¸§: {report['camera_config']['num_train_frames']}
- ç›¸æœºè½¨é“åŠå¾„: {report['camera_config']['trajectory_radius']}
- ç›¸æœºé«˜åº¦: {report['camera_config']['camera_height']}
- æ°´å¹³è§†è§’: {np.degrees(report['camera_config']['camera_angle_x']):.1f}Â°

## è®­ç»ƒå»ºè®®
- æ¨èè¿­ä»£: {report['quality_metrics']['recommended_iterations']:,}
- å»ºè®®å‘½ä»¤:
## ç”Ÿæˆæ—¶é—´
{os.popen('date').read().strip()}
"""
    
    md_path = base_dir / "processing_report.md"
    with open(md_path, 'w') as f:
        f.write(md_report)
    
    print(f"ğŸ“„ MarkdownæŠ¥å‘Š: {md_path}")
    
    return report

def main():
    """ä¸»å‡½æ•°"""
    # è§†é¢‘æ–‡ä»¶è·¯å¾„
    video_path = "/datadisk/home/cv25_010/code/cv_project/2d-gaussian-splatting/kapybara.mp4"
    output_dir = "data/capybara"
    
    print("="*60)
    print("2D-Gaussian Splatting è§†é¢‘å¤„ç† (æ”¹è¿›ç‰ˆ)")
    print("="*60)
    
    # æ£€æŸ¥è§†é¢‘æ–‡ä»¶
    video_path = Path(video_path)
    if not video_path.exists():
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        return
    
    # åˆ†æè§†é¢‘ç¡®å®šæœ€ä½³å‚æ•°
    target_frames = analyze_video_for_best_params(video_path)
    
    print(f"\nğŸ¯ ç›®æ ‡å¸§æ•°: {target_frames}")
    
    # è·å–è§†é¢‘ä¿¡æ¯
    cap = cv2.VideoCapture(str(video_path))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps
    cap.release()
    
    video_info = {
        "path": str(video_path),
        "fps": fps,
        "total_frames": total_frames,
        "duration": duration
    }
    
    # é€‰é¡¹
    print("\né€‰æ‹©å¤„ç†æ¨¡å¼:")
    print("1. è‡ªåŠ¨æ¨¡å¼ (æ¨è)")
    print("2. è‡ªå®šä¹‰å¸§æ•°")
    
    choice = input("\nè¯·é€‰æ‹© (1 æˆ– 2): ").strip()
    
    if choice == "2":
        try:
            custom_frames = int(input(f"è¾“å…¥ç›®æ ‡å¸§æ•° (1-{min(500, total_frames)}): ").strip())
            target_frames = min(max(1, custom_frames), 500)
        except:
            print("ä½¿ç”¨è‡ªåŠ¨æ¨¡å¼")
    
    # å¤„ç†è§†é¢‘
    print(f"\nğŸš€ å¼€å§‹å¤„ç†è§†é¢‘...")
    base_dir, image_files = extract_frames_uniform(
        video_path=video_path,
        output_dir=output_dir,
        target_frames=target_frames
    )
    
    if base_dir and image_files:
        # åˆ›å»ºé…ç½®æ–‡ä»¶
        success = create_precise_camera_poses(
            base_dir=base_dir,
            image_files=image_files,
            camera_angle_x=0.6911112070083618,
            radius=3.0,
            height=1.5,
            look_at=(0, 0, 0)
        )
        
        if success:
            # åˆ›å»ºè´¨é‡æŠ¥å‘Š
            report = create_quality_report(base_dir, image_files, video_info)
            
            print(f"\n" + "="*60)
            print("âœ… å¤„ç†å®Œæˆ!")
            print("="*60)
            
            print(f"\nğŸ“ æ•°æ®é›†è·¯å¾„: {base_dir}")
            print(f"ğŸ“Š æå–å¸§æ•°: {len(image_files)}")
            
            print(f"\nğŸš€ è®­ç»ƒå‘½ä»¤:")
            rec_iter = report['quality_metrics']['recommended_iterations']
            print(f"python train.py -s {base_dir} \\")
            print(f"  -m output/model_improved \\")
            print(f"  --iterations {rec_iter} \\")
            print(f"  --save_iterations {rec_iter//5} {rec_iter//2} {rec_iter} \\")
            print(f"  --resolution 1 \\")
            print(f"  --white_background \\")
            print(f"  --quiet")
            
            print(f"\nğŸ’¡ æç¤º: æ›´å¤šå¸§æ•°éœ€è¦æ›´å¤šè¿­ä»£æ‰èƒ½è·å¾—å¥½æ•ˆæœ")
            print(f"       {len(image_files)}å¸§å»ºè®®ä½¿ç”¨{rec_iter:,}æ¬¡è¿­ä»£")

if __name__ == "__main__":
    # è®¾ç½®matplotlibåç«¯
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    
    main()